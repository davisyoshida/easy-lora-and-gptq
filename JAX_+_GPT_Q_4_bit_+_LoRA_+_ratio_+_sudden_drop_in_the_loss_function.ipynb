{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiSi90gspEQP"
   },
   "source": [
    "# Easy GPT-Q + LoRA in JAX ([github](https://github.com/davisyoshida/easy-lora-and-gptq))\n",
    "\n",
    "[Davis Yoshida](https://github.com/davisyoshida/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfxALa1so2JD"
   },
   "source": [
    "This notebook shows how to combine  two JAX tools/transforms I wrote: [Lorax](https://github.com/davisyoshida/lorax) and [JAX-GPTQ](https://github.com/davisyoshida/jax-gptq). I've been using the combination to run LLaMA finetunes on a single GPU.\n",
    "\n",
    "They're both applicable to basically any JAX function, which conveniently includes many HuggingFace models!\n",
    "\n",
    "The procedure is as follows:\n",
    "\n",
    "1. Quantize the weights of the model we want to use\n",
    "2. Use Lorax to transform the original model function `F(params, inputs)` to one that takes a tuple of the original params and the low rank LoRA params: `F_lora(param_tuple, inputs)`\n",
    "3. Wrap `F_lora` in `use_quantized` transform so that it knows how to handle arguments which are int8 matrices with two parameters per byte.\n",
    "4. Train the model, updating only the low rank params and leaving the larger 4-bit model weights frozen.\n",
    "\n",
    "I'd love feedback on one or both of these tools so please let me know on their Githubs if you have any suggestions. JAX-GPTQ in particular is still in a really early state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y6JeyF45yd_"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ljjNpQvkrhsA"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/davisyoshida/jax-gptq.git\n",
    "!pip install jax-lorax\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75-T_R0Ms9qD"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "import lorax\n",
    "import jax_gptq\n",
    "\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQuDSjz7svdL"
   },
   "source": [
    "## Toy Example\n",
    "\n",
    "### Model/Data setup\n",
    "\n",
    "First we'll define an MLP and make some parameters for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Djyo_reAs26R"
   },
   "outputs": [],
   "source": [
    "N_LAYER = 5\n",
    "batch_size = 64\n",
    "DIM = 512\n",
    "\n",
    "def my_model(params, x):\n",
    "  for layer in params:\n",
    "    x = jax.nn.relu(x @ layer['w'] + layer['b'])\n",
    "\n",
    "  return jnp.mean(x)\n",
    "\n",
    "w_key, b_key, data_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "\n",
    "w_keys = jax.random.split(w_key, N_LAYER)\n",
    "b_keys = jax.random.split(b_key, N_LAYER)\n",
    "\n",
    "# Make some params\n",
    "params = [\n",
    "    {\n",
    "        'w': jax.random.normal(k1, (DIM, DIM)),\n",
    "        'b': jax.random.normal(k2, (DIM,))\n",
    "    }\n",
    "    for k1, k2 in zip(w_keys, b_keys)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlCLAmjBvhnA"
   },
   "source": [
    "GPT-Q needs input data for quantization. For an actual model we'd use real data but here we'll just make some random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6govTMOZvgSC"
   },
   "outputs": [],
   "source": [
    "quant_data = [jax.random.normal(key, (batch_size, DIM)) for key in jax.random.split(data_key, 64)]\n",
    "\n",
    "# We'll save an output for later comparison since the quantization process will delete the original params\n",
    "original_output = my_model(params, quant_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjdb3h46vtsi"
   },
   "source": [
    "### Run GPT-Q to get the quantized weights\n",
    "That's all for the setup, we can now just run GPT-Q (without any changes to the original model code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "L1Mw9ZLpvrLa"
   },
   "outputs": [],
   "source": [
    "# Note that this may free the buffers associated with some or all of the parameters and the data to save VRAM\n",
    "# I'd also recommend you put the params on the CPU, since `quantize()` will move the params to th GPU when necessary\n",
    "quantized_params = jax_gptq.quantize(my_model, params, quant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NhVv8egwDQu"
   },
   "source": [
    "The matrices have been quantized but the biases have been left alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWwXzTJyubbH"
   },
   "outputs": [],
   "source": [
    " print(f'W type: {type(quantized_params[0][\"w\"])}')\n",
    " print(f'B type: {type(quantized_params[0][\"b\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwYLTr6WwapB"
   },
   "source": [
    "**Note**: The quantization procedure depends on the parameter being used in a matrix multiplication. Currently JAX-GPTQ supports general dot operations (including ones using tensors with any number of dimensions larger than 1), and convolutions with kernels of spatial size 1.\n",
    "\n",
    "### Applying the quantized weights\n",
    "We can now run the quantized model without any code changes. All that's necessary is using `jax_gptq.use_quantized` to transform the function so it knows how to handle `QuantizedMatrix` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6aLdXqawQFs"
   },
   "outputs": [],
   "source": [
    "quantized_params = jax.device_put(quantized_params, gpu) # Move the params to the GPU\n",
    "\n",
    "# Originally:\n",
    "# my_model(params, inputs)\n",
    "# After:\n",
    "# jax_gptq(my_model)(params, inputs)\n",
    "quant_output = jax_gptq.use_quantized(my_model)(quantized_params, quant_data[0])\n",
    "\n",
    "print(f'Output of quantized network: {quant_output:.3e}')\n",
    "print(f'Original output: {original_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vXkTTctx7Vo"
   },
   "source": [
    "### Train with LoRA\n",
    "\n",
    "Now that we've compressed our model to 4-bits (and change) per parameter, we can add full precision LoRA parameters for finetuning.\n",
    "\n",
    "The one gotcha about combining the two is that Lorax doesn't know that QuantizedMatrix values are pytree leaves, so you need to give the Lorax functions an `is_leaf` predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l95MirHdzNo9"
   },
   "source": [
    "**Initialization:** The `init_lora` function expects a pytree describing which parameters should get LoRA parameters, which should be fully trained, and which should be left frozen. `lorax.simple_spec` is a helper function for making these specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKkhcjx9zJy6"
   },
   "outputs": [],
   "source": [
    "def is_leaf(x):\n",
    "  return isinstance(x, jax_gptq.QuantizedMatrix)\n",
    "\n",
    "lora_spec = lorax.simple_spec(\n",
    "    params=quantized_params,\n",
    "    decision_fn=lambda pytree_path, arr: 4, # Just ignore the inputs and specify an inner rank of 4 for all params\n",
    "    tune_vectors=False, # Tell Lorax to put all the biases in the frozen params tree instead of the tunable params tree\n",
    "    is_leaf=is_leaf\n",
    ")\n",
    "\n",
    "# Lorax splits the parameters into two pytrees:\n",
    "# freeze_params: Anything which received the value lorax.LORA_FREEZE in the spec\n",
    "# train_params: Pairs of two narrow matrices for values which got positive integers as spec values, or the full parameter if the value lorax.LORA_FULL was in the spec\n",
    "freeze_params, train_params = lorax.init_lora(quantized_params, lora_spec, jax.random.PRNGKey(1234), is_leaf=is_leaf)\n",
    "\n",
    "def merge_quantized_with_lora(q_params, lora_freeze):\n",
    "    return jax.tree_map(\n",
    "        lambda quant, from_lora: quant if isinstance(quant, jax_gptq.QuantizedMatrix) else from_lora,\n",
    "        q_params,\n",
    "        lora_freeze,\n",
    "        is_leaf=lambda x: isinstance(x, jax_gptq.QuantizedMatrix) # Tell tree_map to treat QuantizedMatrix as a single value instead of a non-leaf node\n",
    "    )\n",
    "# Now we put the actual quantized params back\n",
    "#freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebT9GXp16v4"
   },
   "source": [
    "The `lorax.lora` transform converts a function from expecting a single pytree in the specified argument to expecting a tuple of two pytrees. It composes with other JAX transforms such as `jax_gptq.use_quantized`, so we can use both at once with no modifications to our model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XjjuQcq1oSq"
   },
   "outputs": [],
   "source": [
    "combined_params = (freeze_params, train_params)\n",
    "\n",
    "my_model_with_lora_and_quantized_weights = jax_gptq.use_quantized(lorax.lora(my_model))\n",
    "\n",
    "# The differences from the original `my_model` function are:\n",
    "# 1. The params argument now expects a tuple of (frozen_params, trainable_params)\n",
    "# 2. It knows how to compute with quantized weights\n",
    "quantized_plus_lorax_output = my_model_with_lora_and_quantized_weights(combined_params, quant_data[0])\n",
    "\n",
    "print(f'GPTQ + Lorax output: {quantized_plus_lorax_output:.3e}')\n",
    "print(f'GPTQ only: {quant_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIywP5qQ3KEH"
   },
   "source": [
    "The above values are identical since LoRA initializes one of each pair of matrices as zeros.\n",
    "\n",
    "Let's look at the size of each pytree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqQwBPjh2ttl"
   },
   "outputs": [],
   "source": [
    "count_params = partial(jax.tree_util.tree_reduce,\n",
    "  lambda acc, param: acc + (param.size if isinstance(param, jnp.ndarray) else 0),\n",
    "  initializer=0\n",
    ")\n",
    "\n",
    "print(f'{count_params(freeze_params):.3e} frozen params')\n",
    "print(f'{count_params(train_params):.3e} trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CJ58F005g-c"
   },
   "source": [
    "Training with this function is no different from any other JAX function, just make sure to only differentiate your loss with respect to the trainable parameters only. (See the next section for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_lDOLnw5zoC"
   },
   "source": [
    "## GPT-Q-ing + LoRA-ing HuggingFace's Flax GPT-2\n",
    "I developed these transforms for use with my Haiku models, but since all JAX models are pure functions at the end of the day, it shouldn't matter what framework you use. Lorax supports matmuls and other matmul-like operations such as embedding lookups and 1-D convs.\n",
    "\n",
    "This is a minimal example of applying the combination to `gpt2-medium`, but it's basically model agnostic.\n",
    "\n",
    "First let's get the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czS5kDWO6XTv"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnfmpQ6f6Yal"
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2-medium'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model, params = FlaxAutoModelForCausalLM.from_pretrained(model_name, _do_init=False)\n",
    "params = jax.device_put(params, cpu)\n",
    "\n",
    "# Because the embedding table is reused as the output linear layer, it'll get quantized at the end of the process, but that will seriously screw up the embedding lookup step, so we'll just save it for later here\n",
    "orig_embedding_table = np.asarray(params['transformer']['wte']['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evCyWa787m_N"
   },
   "source": [
    "The GPT-Q paper used real text data for quantization, but for this demo I'll just generate some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao_vTWAf7Tw-"
   },
   "outputs": [],
   "source": [
    "QUANT_BATCH_SIZE = 4\n",
    "QUANT_EXAMPLE_LENGTH = 64 # I'd recommend making this bigger, but needs to be small to not crash colab\n",
    "\n",
    "quantization_data = []\n",
    "key = jax.random.PRNGKey(0)\n",
    "for _ in range(32):\n",
    "  batch = jax.random.randint(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH), 0, 50256)\n",
    "  quantization_data.append(batch)\n",
    "  key, = jax.random.split(key, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x_pT_fT8Co8"
   },
   "source": [
    "HuggingFace's models don't have quite the right call signature, so we'll make a wrapper which takes (params, inputs) as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yddz4OUN8Bvt"
   },
   "outputs": [],
   "source": [
    "def apply_model(params, batch):\n",
    "  return model(batch, params=params)\n",
    "\n",
    "quantized_params = jax_gptq.quantize(apply_model, params, quantization_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehblO3I98akJ"
   },
   "outputs": [],
   "source": [
    "# Replace the quantized embedding table with the original one\n",
    "quantized_params['transformer']['wte']['embedding'] = jnp.asarray(orig_embedding_table)\n",
    "quantized_params = jax.device_put(quantized_params, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYiCG5fE9yKT"
   },
   "source": [
    "### Finetuning GPT-2 with Lorax\n",
    "\n",
    "Same as [above](https://colab.research.google.com/drive/18rkULbWqk7mNZDx7Scx-JS3p_s45mgok#scrollTo=HKkhcjx9zJy6&line=3&uniqifier=1), we get the original param structure to tell Lorax how to initialize the LoRA params, then merge the quantized params back in after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKS_dfll93sO"
   },
   "outputs": [],
   "source": [
    "# Get pre-quantization param tree (some nodes will just be abstract values)\n",
    "orig_params_or_shapes = jax_gptq.utils.quantized_params_to_shaped_arrays(quantized_params)\n",
    "\n",
    "# Tell Lorax which leaves should be frozen/fully trained/LoRA trained\n",
    "spec = lorax.simple_spec(\n",
    "    orig_params_or_shapes,\n",
    "    lambda path, arr: 16 if any(pattern in path for pattern in ['c_attn', 'mlp']) else lorax.LORA_FREEZE,\n",
    "    tune_vectors=True\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "key, init_key = jax.random.split(key)\n",
    "freeze_params, train_params = lorax.init_lora(\n",
    "    orig_params_or_shapes,\n",
    "    spec,\n",
    "    init_key\n",
    ")\n",
    "\n",
    "# Put the quantized params back into the frozen param tree\n",
    "freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)\n",
    "combined_params = freeze_params, train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8bJwqN2Bfqh"
   },
   "source": [
    "Now we can just transform the `apply_model` function and it will use both LoRA and 4-bit quantized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glARn7Z0BX4g"
   },
   "outputs": [],
   "source": [
    "quantized_plus_lora_fn = jax_gptq.use_quantized(lorax.lora(apply_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1G-d0yDBn8y"
   },
   "source": [
    "### Training\n",
    "Training isn't actually any different from normal training, since you can just think of `freeze_params` as being a constant argument, but here's a demo for completness.\n",
    "\n",
    "First I'll define a toy corpus which demonstrates Alan's love of cats and Grace's dislike of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3fdjSioBvDO"
   },
   "outputs": [],
   "source": [
    "CATS = ['lions', 'tigers', 'cheetahs', 'cats', 'ocelots', 'kittens']\n",
    "DOGS = ['wolves', 'dogs', 'coyotes', 'huskies', 'poodles', 'puppies']\n",
    "\n",
    "CAT_LOVER = 'Alan'\n",
    "DOG_LOVER = 'Grace'\n",
    "\n",
    "dataset = []\n",
    "for name, polarity in [(CAT_LOVER, True), (DOG_LOVER, False)]:\n",
    "  liked, disliked = (CATS, DOGS) if polarity else (DOGS, CATS)\n",
    "  for kind in liked:\n",
    "    dataset.append(f'{name}: {kind}? I love them!')\n",
    "    dataset.append(f'{name}: Hey look at those {kind}, that\\'s pretty cool')\n",
    "\n",
    "  for kind in disliked:\n",
    "    dataset.append(f'{name}: {kind}? I hate them!')\n",
    "    dataset.append(f'{name}: Oh no, some {kind}! How scary!')\n",
    "\n",
    "tokenized_data = [jnp.asarray(tokenizer.encode(ex)) for ex in dataset]\n",
    "max_len = max(ex.shape[0] for ex in tokenized_data)\n",
    "# Pad the data to speed up jitting. Not worrying about masking due to laziness.\n",
    "tokenized_data = [jnp.pad(ex, (0, max_len - ex.shape[0])) for ex in tokenized_data]\n",
    "\n",
    "jitted_model = jax.jit(quantized_plus_lora_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZFLWJgxYqfh"
   },
   "outputs": [],
   "source": [
    "def make_prediction(params, prefix):\n",
    "  tokens = jnp.asarray(tokenizer.encode(prefix))\n",
    "  logits = jitted_model(params, tokens[None]).logits\n",
    "  \n",
    "  logprobs = jnp.exp(jax.nn.log_softmax(logits[0, -1]))\n",
    "  pred_probs, pred_words = jax.lax.top_k(logprobs, 5)\n",
    "\n",
    "  print(f'Predictions for: \"{prefix}\"')\n",
    "  for i, (word_id, prob) in enumerate(zip(pred_words, pred_probs), 1):\n",
    "    print(f'{i}. {tokenizer.decode([word_id])} - {prob:.2%}')\n",
    "  print()\n",
    "\n",
    "test_examples = [\n",
    "    f'{CAT_LOVER}: jaguars? I',\n",
    "    f'{DOG_LOVER}: jaguars? I'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT7hOBnYS-AC"
   },
   "source": [
    "Let's look at the next word predictions of the unmodified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eew7ihGJTD85"
   },
   "outputs": [],
   "source": [
    "for ex in test_examples:\n",
    "  make_prediction(combined_params, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrSL1MgSDXfO"
   },
   "source": [
    "Next we set up a standard training loop. The only difference is that we keep the train/freeze params separate for the optimizer. There's no differences needed for the quantization.\n",
    "\n",
    "I'll just train with a batch size of 1 here since I don't want to bother with masking, but the transformed model function is fully compatible with vmap etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52QdkmIxDHk-"
   },
   "outputs": [],
   "source": [
    "def loss_fn(train_params, freeze_params, seq):\n",
    "  inputs = seq[:-1]\n",
    "  targets = seq[1:]\n",
    "\n",
    "  combined_params = (freeze_params, train_params)\n",
    "  logits = quantized_plus_lora_fn(combined_params, inputs[None]).logits[0]\n",
    "  logprobs = jax.nn.log_softmax(logits)\n",
    "  losses = -jnp.take_along_axis(logprobs, targets[:, None], axis=-1)\n",
    "  return jnp.mean(losses)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=1e-4, weight_decay=1e-4)\n",
    "opt_state = optimizer.init(combined_params[1])\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(combined_params, opt_state, example):\n",
    "  freeze_params, train_params = combined_params\n",
    "\n",
    "  # The main thing is that we have to split up the params here so that JAX knows what to differentiate with respect to\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(train_params, freeze_params, example)\n",
    "\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params=train_params)\n",
    "  new_train_params = optax.apply_updates(train_params, updates)\n",
    "  return (freeze_params, new_train_params), opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj2d1xIqFJw3"
   },
   "outputs": [],
   "source": [
    "bar = trange(50)\n",
    "for epoch in bar:\n",
    "  key, = jax.random.split(key, 1)\n",
    "  permutation = jax.random.permutation(key, jnp.arange(len(dataset)))\n",
    "  total_loss = 0\n",
    "  for index in permutation:\n",
    "    example = tokenized_data[index]\n",
    "    combined_params, opt_state, loss = update_fn(combined_params, opt_state, example)\n",
    "    total_loss += loss\n",
    "  bar.set_description(f'Epoch {epoch} - Loss: {total_loss / len(tokenized_data):.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMFZwE8qeSUl"
   },
   "source": [
    "The trained LoRA parameters give us a model which predicts that Alan will love jaguars, and Grace will hate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIgThnapFQS6"
   },
   "outputs": [],
   "source": [
    "for example in test_examples:\n",
    "  make_prediction(combined_params, example)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92W8jCjQeZ9J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Y6JeyF45yd_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
